{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Spark notebook ###\n",
    "\n",
    "This notebook will only work in a Jupyter session running on `mathmadslinux2p`.\n",
    "\n",
    "You can start your own Jupyter session on `mathmadslinux2p` and open this notebook in Chrome on the MADS Windows server by\n",
    "\n",
    "**Steps**\n",
    "\n",
    "1. Login to the MADS Windows server using https://mathportal.canterbury.ac.nz/.\n",
    "2. Download or copy this notebook to your home directory.\n",
    "3. Open powershell and run `ssh mathmadslinux2p`.\n",
    "4. Run `start_pyspark_notebook` or `/opt/anaconda3/bin/jupyter-notebook --ip 132.181.129.68 --port $((8000 + $((RANDOM % 999))))`.\n",
    "5. Copy / paste the url provided in the shell window into Chrome on the MADS Windows server.\n",
    "6. Open the notebook from the Jupyter root directory (which is your home directory).\n",
    "7. Run `start_spark()` to start a spark session in the notebook.\n",
    "8. Run `stop_spark()` before closing the notebook or kill your spark application by hand using the link in the Spark UI."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>pre { white-space: pre !important; }table.dataframe td { white-space: nowrap !important; }table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }</style>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pandas\n",
    "import pyspark\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example notebook ###\n",
    "\n",
    "The code below provides a template for how you would use a notebook to start spark, run some code, and then stop spark.\n",
    "\n",
    "**Steps**\n",
    "\n",
    "- Run `start_spark()` to start a spark session in the notebook (only change the default resources when advised to do so for an exercise or assignment)\n",
    "- Write and run code interactively, creating additional cells as needed.\n",
    "- Run `stop_spark()` before closing the notebook or kill your spark application by hand using the link in the [Spark UI](http://mathmadslinux2p.canterbury.ac.nz:8080/)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>dca129 (jupyter)</code> under the running applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:4145\" target=\"_blank\">Spark Application UI</a></li></ul><p><b>Config</b></p><table width=\"100%\" style=\"width:100%; font-family: monospace;\"><tr><td style=\"text-align:left;\">spark.dynamicAllocation.enabled</td><td>false</td></tr><tr><td style=\"text-align:left;\">spark.driver.port</td><td>33569</td></tr><tr><td style=\"text-align:left;\">spark.master</td><td>spark://masternode2:7077</td></tr><tr><td style=\"text-align:left;\">spark.app.id</td><td>app-20240914192126-0945</td></tr><tr><td style=\"text-align:left;\">spark.executor.id</td><td>driver</td></tr><tr><td style=\"text-align:left;\">spark.sql.warehouse.dir</td><td>file:/users/home/dca129/Assignment1/spark-warehouse</td></tr><tr><td style=\"text-align:left;\">spark.driver.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.driver.host</td><td>mathmadslinux2p.canterbury.ac.nz</td></tr><tr><td style=\"text-align:left;\">spark.ui.port</td><td>4145</td></tr><tr><td style=\"text-align:left;\">spark.driver.extraJavaOptions</td><td>-Dderby.system.home=/tmp/dca129/spark/</td></tr><tr><td style=\"text-align:left;\">spark.executor.memory</td><td>1g</td></tr><tr><td style=\"text-align:left;\">spark.rdd.compress</td><td>True</td></tr><tr><td style=\"text-align:left;\">spark.app.name</td><td>dca129 (jupyter)</td></tr><tr><td style=\"text-align:left;\">spark.executor.instances</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.serializer.objectStreamReset</td><td>100</td></tr><tr><td style=\"text-align:left;\">spark.driver.maxResultSize</td><td>0</td></tr><tr><td style=\"text-align:left;\">spark.cores.max</td><td>2</td></tr><tr><td style=\"text-align:left;\">spark.submit.pyFiles</td><td></td></tr><tr><td style=\"text-align:left;\">spark.executor.cores</td><td>1</td></tr><tr><td style=\"text-align:left;\">spark.submit.deployMode</td><td>client</td></tr><tr><td style=\"text-align:left;\">spark.app.startTime</td><td>1726298485745</td></tr><tr><td style=\"text-align:left;\">spark.sql.shuffle.partitions</td><td>8</td></tr><tr><td style=\"text-align:left;\">spark.ui.showConsoleProgress</td><td>true</td></tr></table><p><b>Notes</b></p><ul><li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li><li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>dca129 (jupyter)</code> by hand using the link in the Spark UI.</li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.a"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 263 items\r\n",
      "-rw-r--r--   8 jsw93 jsw93    1897634 2024-08-07 01:43 /data/ghcnd/daily/1750.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3358 2024-08-07 01:44 /data/ghcnd/daily/1763.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3327 2024-08-07 01:42 /data/ghcnd/daily/1764.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3335 2024-08-07 01:41 /data/ghcnd/daily/1765.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3344 2024-08-07 01:38 /data/ghcnd/daily/1766.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3356 2024-08-07 01:43 /data/ghcnd/daily/1767.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3325 2024-08-07 01:41 /data/ghcnd/daily/1768.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3418 2024-08-07 01:41 /data/ghcnd/daily/1769.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93       3357 2024-08-07 01:43 /data/ghcnd/daily/1770.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls /data/ghcnd/daily | head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-rw-r--r--   8 jsw93 jsw93    156.8 M 2024-08-07 01:40 /data/ghcnd/daily/2015.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    158.1 M 2024-08-07 01:42 /data/ghcnd/daily/2016.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    157.7 M 2024-08-07 01:43 /data/ghcnd/daily/2017.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    157.8 M 2024-08-07 01:41 /data/ghcnd/daily/2018.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    156.7 M 2024-08-07 01:40 /data/ghcnd/daily/2019.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    157.6 M 2024-08-07 01:43 /data/ghcnd/daily/2020.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    160.3 M 2024-08-07 01:38 /data/ghcnd/daily/2021.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    161.1 M 2024-08-07 01:42 /data/ghcnd/daily/2022.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93    160.6 M 2024-08-07 01:41 /data/ghcnd/daily/2023.csv.gz\r\n",
      "-rw-r--r--   8 jsw93 jsw93     84.7 M 2024-08-07 01:42 /data/ghcnd/daily/2024.csv.gz\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs dfs -ls -h /data/ghcnd/daily | tail"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs getconf -confKey \"dfs.blocksize\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "134217728\n"
     ]
    }
   ],
   "source": [
    "print(128*1024**2) #MB - KB -B"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=dca129&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2024.csv.gz\r\n",
      "FSCK started by dca129 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2024.csv.gz at Sat Sep 14 19:23:30 NZST 2024\r\n",
      "\r\n",
      "/data/ghcnd/daily/2024.csv.gz 88831735 bytes, replicated: replication=8, 1 block(s):  OK\r\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220563_479763 len=88831735 Live_repl=8\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t32\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t88831735 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t1 (avg. block size 88831735 B)\r\n",
      " Minimally replicated blocks:\t1 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t4\r\n",
      " Average block replication:\t8.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Sat Sep 14 19:23:30 NZST 2024 in 1 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/data/ghcnd/daily/2024.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /data/ghcnd/daily/2024.csv.gz -files -blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Connecting to namenode via http://masternode2:9870/fsck?ugi=ywa244&files=1&blocks=1&path=%2Fdata%2Fghcnd%2Fdaily%2F2023.csv.gz\r\n",
      "FSCK started by ywa244 (auth:SIMPLE) from /192.168.40.11 for path /data/ghcnd/daily/2023.csv.gz at Mon Sep 09 12:07:53 NZST 2024\r\n",
      "\r\n",
      "/data/ghcnd/daily/2023.csv.gz 168357302 bytes, replicated: replication=8, 2 block(s):  OK\r\n",
      "0. BP-700027894-132.181.129.68-1626517177804:blk_1074220535_479735 len=134217728 Live_repl=8\r\n",
      "1. BP-700027894-132.181.129.68-1626517177804:blk_1074220536_479736 len=34139574 Live_repl=8\r\n",
      "\r\n",
      "\r\n",
      "Status: HEALTHY\r\n",
      " Number of data-nodes:\t32\r\n",
      " Number of racks:\t\t1\r\n",
      " Total dirs:\t\t\t0\r\n",
      " Total symlinks:\t\t0\r\n",
      "\r\n",
      "Replicated Blocks:\r\n",
      " Total size:\t168357302 B\r\n",
      " Total files:\t1\r\n",
      " Total blocks (validated):\t2 (avg. block size 84178651 B)\r\n",
      " Minimally replicated blocks:\t2 (100.0 %)\r\n",
      " Over-replicated blocks:\t0 (0.0 %)\r\n",
      " Under-replicated blocks:\t0 (0.0 %)\r\n",
      " Mis-replicated blocks:\t\t0 (0.0 %)\r\n",
      " Default replication factor:\t4\r\n",
      " Average block replication:\t8.0\r\n",
      " Missing blocks:\t\t0\r\n",
      " Corrupt blocks:\t\t0\r\n",
      " Missing replicas:\t\t0 (0.0 %)\r\n",
      " Blocks queued for replication:\t0\r\n",
      "\r\n",
      "Erasure Coded Block Groups:\r\n",
      " Total size:\t0 B\r\n",
      " Total files:\t0\r\n",
      " Total block groups (validated):\t0\r\n",
      " Minimally erasure-coded block groups:\t0\r\n",
      " Over-erasure-coded block groups:\t0\r\n",
      " Under-erasure-coded block groups:\t0\r\n",
      " Unsatisfactory placement block groups:\t0\r\n",
      " Average block group size:\t0.0\r\n",
      " Missing block groups:\t\t0\r\n",
      " Corrupt block groups:\t\t0\r\n",
      " Missing internal blocks:\t0\r\n",
      " Blocks queued for replication:\t0\r\n",
      "FSCK ended at Mon Sep 09 12:07:53 NZST 2024 in 2 milliseconds\r\n",
      "\r\n",
      "\r\n",
      "The filesystem under path '/data/ghcnd/daily/2023.csv.gz' is HEALTHY\r\n"
     ]
    }
   ],
   "source": [
    "!hdfs fsck /data/ghcnd/daily/2023.csv.gz -files -blocks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.b"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "37867272\n"
     ]
    }
   ],
   "source": [
    "daily_2023 = spark.read.csv(\"hdfs:///data/ghcnd/daily/2023.csv.gz\")\n",
    "\n",
    "print(daily_2023.rdd.getNumPartitions())\n",
    "print(daily_2023.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1\n",
      "19720790\n"
     ]
    }
   ],
   "source": [
    "daily_2024 = spark.read.csv(\"hdfs:///data/ghcnd/daily/2024.csv.gz\")\n",
    "\n",
    "print(daily_2024.rdd.getNumPartitions())\n",
    "print(daily_2024.count())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "370803270\n"
     ]
    }
   ],
   "source": [
    "data = spark.read.csv(\"hdfs:///data/ghcnd/daily/{2014,2015,2016,2017,2018,2019,2020,2021,2022,2023}*.csv.gz\")\n",
    "total_observations = data.count()\n",
    "print(total_observations)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "print(data.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Q1.d"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "105\n"
     ]
    }
   ],
   "source": [
    "daily_total = spark.read.csv(\"hdfs:///data/ghcnd/daily\")\n",
    "print(daily_total.rdd.getNumPartitions())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<p><b>Spark</b></p><p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>dca129 (jupyter)</code> is under the completed applications section in the Spark UI.</p><ul><li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li></ul>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
