{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to import pyspark and to define start_spark() and stop_spark()\n",
    "\n",
    "import findspark\n",
    "\n",
    "findspark.init()\n",
    "\n",
    "import getpass\n",
    "import pyspark\n",
    "import pandas\n",
    "import random\n",
    "import re\n",
    "\n",
    "from IPython.display import display, HTML\n",
    "from pyspark import SparkContext\n",
    "from pyspark.sql import SparkSession\n",
    "\n",
    "\n",
    "# Functions used below\n",
    "\n",
    "def username():\n",
    "    \"\"\"Get username with any domain information removed.\n",
    "    \"\"\"\n",
    "\n",
    "    return re.sub('@.*', '', getpass.getuser())\n",
    "\n",
    "\n",
    "def dict_to_html(d):\n",
    "    \"\"\"Convert a Python dictionary into a two column table for display.\n",
    "    \"\"\"\n",
    "\n",
    "    html = []\n",
    "\n",
    "    html.append(f'<table width=\"100%\" style=\"width:100%; font-family: monospace;\">')\n",
    "    for k, v in d.items():\n",
    "        html.append(f'<tr><td style=\"text-align:left;\">{k}</td><td>{v}</td></tr>')\n",
    "    html.append(f'</table>')\n",
    "\n",
    "    return ''.join(html)\n",
    "\n",
    "\n",
    "def show_as_html(df, n=20):\n",
    "    \"\"\"Leverage existing pandas jupyter integration to show a spark dataframe as html.\n",
    "    \n",
    "    Args:\n",
    "        n (int): number of rows to show (default: 20)\n",
    "    \"\"\"\n",
    "\n",
    "    display(df.limit(n).toPandas())\n",
    "\n",
    "    \n",
    "def display_spark():\n",
    "    \"\"\"Display the status of the active Spark session if one is currently running.\n",
    "    \"\"\"\n",
    "    \n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        name = sc.getConf().get(\"spark.app.name\")\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:green\">active</span></b>, look for <code>{name}</code> under the running applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'<li><a href=\"{sc.uiWebUrl}\" target=\"_blank\">Spark Application UI</a></li>',\n",
    "            f'</ul>',\n",
    "            f'<p><b>Config</b></p>',\n",
    "            dict_to_html(dict(sc.getConf().getAll())),\n",
    "            f'<p><b>Notes</b></p>',\n",
    "            f'<ul>',\n",
    "            f'<li>The spark session <code>spark</code> and spark context <code>sc</code> global variables have been defined by <code>start_spark()</code>.</li>',\n",
    "            f'<li>Please run <code>stop_spark()</code> before closing the notebook or restarting the kernel or kill <code>{name}</code> by hand using the link in the Spark UI.</li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "        \n",
    "    else:\n",
    "        \n",
    "        html = [\n",
    "            f'<p><b>Spark</b></p>',\n",
    "            f'<p>The spark session is <b><span style=\"color:red\">stopped</span></b>, confirm that <code>{username() + \" (jupyter)\"}</code> is under the completed applications section in the Spark UI.</p>',\n",
    "            f'<ul>',\n",
    "            f'<li><a href=\"http://mathmadslinux2p.canterbury.ac.nz:8080/\" target=\"_blank\">Spark UI</a></li>',\n",
    "            f'</ul>',\n",
    "        ]\n",
    "        display(HTML(''.join(html)))\n",
    "\n",
    "\n",
    "# Functions to start and stop spark\n",
    "\n",
    "def start_spark(executor_instances=2, executor_cores=1, worker_memory=1, master_memory=1):\n",
    "    \"\"\"Start a new Spark session and define globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \n",
    "    Args:\n",
    "        executor_instances (int): number of executors (default: 2)\n",
    "        executor_cores (int): number of cores per executor (default: 1)\n",
    "        worker_memory (float): worker memory (default: 1)\n",
    "        master_memory (float): master memory (default: 1)\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    user = username()\n",
    "    \n",
    "    cores = executor_instances * executor_cores\n",
    "    partitions = cores * 4\n",
    "    port = 4000 + random.randint(1, 999)\n",
    "\n",
    "    spark = (\n",
    "        SparkSession.builder\n",
    "        .master(\"spark://masternode2:7077\")\n",
    "        .config(\"spark.driver.extraJavaOptions\", f\"-Dderby.system.home=/tmp/{user}/spark/\")\n",
    "        .config(\"spark.dynamicAllocation.enabled\", \"false\")\n",
    "        .config(\"spark.executor.instances\", str(executor_instances))\n",
    "        .config(\"spark.executor.cores\", str(executor_cores))\n",
    "        .config(\"spark.cores.max\", str(cores))\n",
    "        .config(\"spark.executor.memory\", f\"{worker_memory}g\")\n",
    "        .config(\"spark.driver.memory\", f\"{master_memory}g\")\n",
    "        .config(\"spark.driver.maxResultSize\", \"0\")\n",
    "        .config(\"spark.sql.shuffle.partitions\", str(partitions))\n",
    "        .config(\"spark.ui.port\", str(port))\n",
    "        .appName(user + \" (jupyter)\")\n",
    "        .getOrCreate()\n",
    "    )\n",
    "    sc = SparkContext.getOrCreate()\n",
    "    \n",
    "    display_spark()\n",
    "\n",
    "    \n",
    "def stop_spark():\n",
    "    \"\"\"Stop the active Spark session and delete globals for SparkSession (spark) and SparkContext (sc).\n",
    "    \"\"\"\n",
    "\n",
    "    global spark\n",
    "    global sc\n",
    "\n",
    "    if 'spark' in globals() and 'sc' in globals():\n",
    "\n",
    "        spark.stop()\n",
    "\n",
    "        del spark\n",
    "        del sc\n",
    "\n",
    "    display_spark()\n",
    "\n",
    "\n",
    "# Make css changes to improve spark output readability\n",
    "\n",
    "html = [\n",
    "    '<style>',\n",
    "    'pre { white-space: pre !important; }',\n",
    "    'table.dataframe td { white-space: nowrap !important; }',\n",
    "    'table.dataframe thead th:first-child, table.dataframe tbody th { display: none; }',\n",
    "    '</style>',\n",
    "]\n",
    "display(HTML(''.join(html)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell to start a spark session in this notebook\n",
    "\n",
    "start_spark(executor_instances=4, executor_cores=2, worker_memory=4, master_memory=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Write your imports here or insert cells below\n",
    "\n",
    "from pyspark.sql import Row, DataFrame, Window, functions as F\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.functions import substring, countDistinct, collect_set, broadcast"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# daily schema\n",
    "schema = StructType([\n",
    "    StructField(\"ID\", StringType(), False),\n",
    "    StructField(\"DATE\", StringType(), False),\n",
    "    StructField(\"ELEMENT\", StringType(), False),\n",
    "    StructField(\"VALUE\", FloatType(), True),\n",
    "    StructField(\"MEASUREMENT FLAG\", StringType(), True),\n",
    "    StructField(\"QUALITY FLAG\", StringType(), True),\n",
    "    StructField(\"SOURCE FLAG\", StringType(), True),\n",
    "    StructField(\"OBSERVATION TIME\", StringType(), True),\n",
    "])\n",
    "daily = spark.read.csv(\"hdfs:///data/ghcnd/daily\", schema=schema)\n",
    "daily.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_nz = daily.where(F.substring(F.col(\"ID\"), 1, 2) == \"NZ\")\n",
    "\n",
    "show_as_html(daily_nz, 20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_tmax_daily = (\n",
    "    daily_nz\n",
    "    .filter(F.col(\"ELEMENT\").isin(\"TMIN\", \"TMAX\") & F.col(\"value\").isNotNull())\n",
    ")\n",
    "show_as_html(tmin_tmax_daily)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Group by ELEMENT (TMIN and TMAX) and count the number of observations for each\n",
    "element_count = (\n",
    "    tmin_tmax_daily\n",
    "    .groupBy(\"ELEMENT\")\n",
    "    .agg(F.count(\"VALUE\").alias(\"observation_count\"))\n",
    ")\n",
    "\n",
    "\n",
    "element_count.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##TMIN|           235380|\n",
    "##TMAX|           252380|"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_tmax_daily_with_year = tmin_tmax_daily.withColumn(\"year\", F.substring(\"DATE\", 1, 4).cast(\"int\"))\n",
    "\n",
    "\n",
    "min_year = tmin_tmax_daily_with_year.agg(F.min(\"year\")).collect()[0][0]\n",
    "max_year = tmin_tmax_daily_with_year.agg(F.max(\"year\")).collect()[0][0]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "print(f\"Years covered in the plot: {min_year} to {max_year}\")\n",
    "##1940 to 2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tmin_tmax_daily_with_year = tmin_tmax_daily.withColumn(\"year\", F.substring(\"DATE\", 1, 4).cast(\"int\"))\n",
    "show_as_html(tmin_tmax_daily_with_year)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tmin_tmax_pandas.to_csv(\"data.csv\", header=True,index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "tmin_tmax_pandas = tmin_tmax_daily_with_year.select(\"ID\", \"DATE\", \"ELEMENT\", \"VALUE\", \"year\").toPandas()\n",
    "\n",
    "# Convert DATE to datetime format in pandas for plotting\n",
    "tmin_tmax_pandas['DATE'] = pd.to_datetime(tmin_tmax_pandas['DATE'], format='%Y%m%d')\n",
    "\n",
    "# Divide VALUES by 10 \n",
    "tmin_tmax_pandas['VALUE'] = tmin_tmax_pandas['VALUE'] / 10\n",
    "\n",
    "#  Filling gaps in case of missing data\n",
    "def fill_missing_dates(df):\n",
    "    full_range = pd.date_range(start=df['DATE'].min(), end=df['DATE'].max(), freq='D')\n",
    "    df = df.set_index('DATE').reindex(full_range).rename_axis('DATE').reset_index()\n",
    "    return df\n",
    "\n",
    "tmin_tmax_pandas = tmin_tmax_pandas.groupby(['ID', 'ELEMENT']).apply(fill_missing_dates).reset_index(drop=True)\n",
    "\n",
    "# Apply rolling average to smooth the time series\n",
    "def apply_rolling_average(group):\n",
    "    group = group.copy()\n",
    "    group['VALUE'] = group['VALUE'].rolling(window=7, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "tmin_tmax_pandas = tmin_tmax_pandas.groupby('ELEMENT').apply(apply_rolling_average).reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Plotting the graphs\n",
    "\n",
    "stations = tmin_tmax_pandas['ID'].unique()\n",
    "\n",
    "for station in stations:\n",
    "    station_data = tmin_tmax_pandas[tmin_tmax_pandas['ID'] == station]\n",
    "    \n",
    "    tmin_data = station_data[station_data['ELEMENT'] == 'TMIN']\n",
    "    tmax_data = station_data[station_data['ELEMENT'] == 'TMAX']\n",
    "    \n",
    "    # Create a new graph for each station\n",
    "    fig = go.Figure()\n",
    "\n",
    "    # Add TMIN trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=tmin_data['DATE'],\n",
    "        y=tmin_data['VALUE'],\n",
    "        mode='lines',\n",
    "        name='TMIN',\n",
    "        line=dict(color='orange', width=2)\n",
    "    ))\n",
    "\n",
    "    # Add TMAX trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=tmax_data['DATE'],\n",
    "        y=tmax_data['VALUE'],\n",
    "        mode='lines',\n",
    "        name='TMAX',\n",
    "        line=dict(color='blue', width=2)\n",
    "    ))\n",
    "\n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        title=f'TMIN and TMAX Over Time for Station {station}',\n",
    "        xaxis_title='Date',\n",
    "        yaxis_title='Temperature (°C)',\n",
    "        legend_title=\"Temperature Type\",\n",
    "        hovermode='x unified',\n",
    "        template='plotly_white',\n",
    "        font=dict(size=12)\n",
    "    )\n",
    "\n",
    "    fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"fig.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "\n",
    "# Convert to Pandas DataFrame\n",
    "tmin_tmax_pandas = tmin_tmax_daily_with_year.select(\"ID\", \"DATE\", \"ELEMENT\", \"VALUE\", \"year\").toPandas()\n",
    "\n",
    "# Convert DATE to datetime format in pandas for plotting\n",
    "tmin_tmax_pandas['DATE'] = pd.to_datetime(tmin_tmax_pandas['DATE'], format='%Y%m%d')\n",
    "\n",
    "# Divide all VALUE entries by 10\n",
    "tmin_tmax_pandas['VALUE'] = tmin_tmax_pandas['VALUE'] / 10\n",
    "\n",
    "# Handle missing data by filling gaps\n",
    "def fill_missing_dates(df):\n",
    "    full_range = pd.date_range(start=df['DATE'].min(), end=df['DATE'].max(), freq='D')\n",
    "    df = df.set_index('DATE').reindex(full_range).rename_axis('DATE').reset_index()\n",
    "    return df\n",
    "\n",
    "tmin_tmax_pandas = tmin_tmax_pandas.groupby(['ID', 'ELEMENT']).apply(fill_missing_dates).reset_index(drop=True)\n",
    "\n",
    "# Apply rolling average to smooth the time series\n",
    "def apply_rolling_average(group):\n",
    "    group = group.copy()\n",
    "    group['VALUE'] = group['VALUE'].rolling(window=7, min_periods=1).mean()\n",
    "    return group\n",
    "\n",
    "tmin_tmax_pandas = tmin_tmax_pandas.groupby('ELEMENT').apply(apply_rolling_average).reset_index(drop=True)\n",
    "\n",
    "# Plotting\n",
    "stations = tmin_tmax_pandas['ID'].unique()\n",
    "num_stations = len(stations)\n",
    "\n",
    "# Define number of rows and columns for the subplots\n",
    "num_cols = 3  \n",
    "num_rows = (num_stations + num_cols - 1) // num_cols  \n",
    "\n",
    "# Create subplot figure\n",
    "fig = make_subplots(rows=num_rows, cols=num_cols, subplot_titles=[f'Station {station}' for station in stations])\n",
    "\n",
    "for idx, station in enumerate(stations):\n",
    "    row = idx // num_cols + 1\n",
    "    col = idx % num_cols + 1\n",
    "\n",
    "    station_data = tmin_tmax_pandas[tmin_tmax_pandas['ID'] == station]\n",
    "    \n",
    "    tmin_data = station_data[station_data['ELEMENT'] == 'TMIN']\n",
    "    tmax_data = station_data[station_data['ELEMENT'] == 'TMAX']\n",
    "    \n",
    "    # Add TMIN trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=tmin_data['DATE'],\n",
    "        y=tmin_data['VALUE'],\n",
    "        mode='lines',\n",
    "        name='TMIN',\n",
    "        line=dict(color='orange', width=2),\n",
    "        showlegend=False  # Hide legend in individual subplots\n",
    "    ), row=row, col=col)\n",
    "\n",
    "    # Add TMAX trace\n",
    "    fig.add_trace(go.Scatter(\n",
    "        x=tmax_data['DATE'],\n",
    "        y=tmax_data['VALUE'],\n",
    "        mode='lines',\n",
    "        name='TMAX',\n",
    "        line=dict(color='blue', width=2),\n",
    "        showlegend=False  # Hide legend in individual subplots\n",
    "    ), row=row, col=col)\n",
    "\n",
    "# Update layout\n",
    "fig.update_layout(\n",
    "    title='TMIN and TMAX Over Time for Each Station',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Temperature (°C)',\n",
    "    legend_title=\"Temperature Type\",\n",
    "    hovermode='x unified',\n",
    "    template='plotly_white',\n",
    "    font=dict(size=12),\n",
    "    height=600 + 200 * num_rows,  # Adjust height based on number of rows\n",
    "    showlegend=True  # Show legend in the overall figure\n",
    ")\n",
    "\n",
    "# Adjust x-axis and y-axis titles to show for the first column and first row only\n",
    "for i in range(num_cols):\n",
    "    fig.update_xaxes(title_text='Date', row=num_rows, col=i + 1)\n",
    "for i in range(num_rows):\n",
    "    fig.update_yaxes(title_text='Temperature (°C)', row=i + 1, col=1)\n",
    "\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig.write_image(\"fig.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import pandas as pd\n",
    "import plotly.graph_objects as go\n",
    "\n",
    "# Assuming `tmin_tmax_daily_with_year` is a Spark DataFrame and has been converted to Pandas for plotting\n",
    "# Convert to Pandas DataFrame\n",
    "tmin_tmax_pandas = tmin_tmax_daily_with_year.select(\"DATE\", \"ELEMENT\", \"VALUE\", \"year\").toPandas()\n",
    "\n",
    "# Convert DATE to datetime format in pandas for plotting\n",
    "tmin_tmax_pandas['DATE'] = pd.to_datetime(tmin_tmax_pandas['DATE'], format='%Y%m%d')\n",
    "\n",
    "# Divide all VALUE entries by 10\n",
    "tmin_tmax_pandas['VALUE'] = tmin_tmax_pandas['VALUE'] / 10\n",
    "\n",
    "# Step 1: Calculate the average TMIN and TMAX for each year\n",
    "# Pivot the data to have 'TMIN' and 'TMAX' in separate columns\n",
    "pivot_data = tmin_tmax_pandas.pivot_table(\n",
    "    index='DATE', \n",
    "    columns='ELEMENT', \n",
    "    values='VALUE', \n",
    "    aggfunc='mean'\n",
    ").reset_index()\n",
    "\n",
    "# Rename columns for easier reference\n",
    "pivot_data.columns.name = None  # Remove the columns name\n",
    "pivot_data.rename(columns={'TMIN': 'Average_TMIN', 'TMAX': 'Average_TMAX'}, inplace=True)\n",
    "\n",
    "# Step 2: Plot the average time series\n",
    "fig1 = go.Figure()\n",
    "\n",
    "# Add average TMIN trace\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=pivot_data['DATE'],\n",
    "    y=pivot_data['Average_TMIN'],\n",
    "    mode='lines',  # Use lines to connect the points\n",
    "    name='Average TMIN',\n",
    "    line=dict(color='orange')\n",
    "))\n",
    "\n",
    "# Add average TMAX trace\n",
    "fig1.add_trace(go.Scatter(\n",
    "    x=pivot_data['DATE'],\n",
    "    y=pivot_data['Average_TMAX'],\n",
    "    mode='lines',  # Use lines to connect the points\n",
    "    name='Average TMAX',\n",
    "    line=dict(color='blue')\n",
    "))\n",
    "\n",
    "# Step 3: Customize the layout\n",
    "fig1.update_layout(\n",
    "    title='Average TMIN and TMAX Over Time for New Zealand',\n",
    "    xaxis_title='Date',\n",
    "    yaxis_title='Temperature (°C)',\n",
    "    legend_title=\"Temperature Type\",\n",
    "    hovermode='x unified'  # To show TMIN and TMAX values together on hover\n",
    ")\n",
    "\n",
    "# Show the consolidated plot\n",
    "fig1.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig1.write_image(\"fig1.jpeg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell before closing the notebook or kill your spark application by hand using the link in the Spark UI\n",
    "\n",
    "stop_spark()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
